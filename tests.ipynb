{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p00000\n",
      "../Project_CodeNet/problem_descriptions/p00000.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"/cs/student/msc/dsml/2023/mdavudov/NLP/Project_CodeNet/problem_descriptions/p02917.html\") as f:\n",
    "    data = f.read()\n",
    "sample = \"../Project_CodeNet/data/p00000/C++/s284212099.cpp\"\n",
    "problem_id = sample.strip('\\n').split(\"/\")[-3]\n",
    "\n",
    "pd_path = f\"../Project_CodeNet/problem_descriptions/{problem_id}.html\"\n",
    "print(pd_path)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score : 300 points\n",
      "Problem Statement\n",
      "There is an integer sequence A of length N whose values are unknown.\n",
      "Given is an integer sequence B of length N-1 which is known to satisfy the following:\n",
      " B_i ≥ max(A_i, A_{i+1}) \n",
      "Find the maximum possible sum of the elements of A.\n",
      "Constraints\n",
      "All values in input are integers.\n",
      "2 ≤ N ≤ 100\n",
      "0 ≤ B_i ≤ 10^5\n",
      "Input\n",
      "Input is given from Standard Input in the following format:\n",
      "N\n",
      "B_1 B_2 ... B_{N-1}\n",
      "Output\n",
      "Print the maximum possible sum of the elements of A.\n",
      "Sample Input 1\n",
      "3\n",
      "2 5\n",
      "Sample Output 1\n",
      "9\n",
      "A can be, for example, ( 2 , 1 , 5 ), ( -1 , -2 , -3 ), or ( 2 , 2 , 5 ). Among those candidates, A = ( 2 , 2 , 5 ) has the maximum possible sum.\n",
      "Sample Input 2\n",
      "2\n",
      "3\n",
      "Sample Output 2\n",
      "6\n",
      "Sample Input 3\n",
      "6\n",
      "0 153 10 10 23\n",
      "Sample Output 3\n",
      "53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "# Find all headings and insert newlines if necessary\n",
    "for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    if '\\n' not in heading.next_sibling:  # Check if next element already has a newline\n",
    "        heading.insert_after(BeautifulSoup('\\n', 'html.parser')) \n",
    "\n",
    "text = soup.get_text()\n",
    "\n",
    "# remove multiple newlines and just have one\n",
    "#text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "text = re.sub(r'\\n{1,}', '\\n', text)\n",
    "\n",
    "symbol_replacements = {\n",
    "    r'\\\\leq': '≤', \n",
    "    r'\\\\geq': '≥', \n",
    "    r'\\\\max': 'max',\n",
    "    r'\\\\min': 'min',\n",
    "    # ... \n",
    "}\n",
    "\n",
    "for pattern, replacement in symbol_replacements.items():\n",
    "    text = re.sub(pattern, replacement, text)\n",
    "\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add promp template code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instruction]\n",
      "You are an assistant that helps users with writing compiler-friendly C++ programmes. Your outputs should be exclusively C++ programmes that can be compiled with C++17. \n",
      "Complete the code given in the input to produce a valid C++ program. The code to complete will be delimited by [C++] and [/C++]. Your response will be delimited with [Response] and [/Response].\n",
      "\n",
      "\n",
      "[Example]\n",
      "\n",
      "[C++]\n",
      "#include <iostream>\n",
      "\n",
      "using namespace std; \n",
      "\n",
      "int main() {\n",
      "    cout << \"Hello, \n",
      "[/C++]\n",
      "\n",
      "[Response]\n",
      "#include <iostream>\n",
      "\n",
      "using namespace std; \n",
      "\n",
      "int main() {\n",
      "    cout << \"Hello, World! << endl;\n",
      "}\n",
      "[/Response]\n",
      "\n",
      "[/Example]\n",
      "\n",
      "[Code Problem Description]\n",
      "\n",
      "Digit Number\n",
      "Write a program which computes the digit number of sum of two integers a and b.\n",
      "Input\n",
      "There are several test cases. Each test case consists of two non-negative integers a and b which are separeted by a space in a line. The input terminates with EOF.\n",
      "Constraints\n",
      "0 ≤ a, b ≤ 1,000,000\n",
      "The number of datasets ≤ 200\n",
      "Output\n",
      "Print the number of digits of a + b for each data set.\n",
      "Sample Input\n",
      "5 7\n",
      "1 99\n",
      "1000 999\n",
      "Output for the Sample Input\n",
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "[/Code Problem Description]\n",
      "[/Instruction]\n",
      "[C++]\n",
      "#include <iostream>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "#include <cstring>\n",
      "#include <climits>\n",
      "#include <algorithm>\n",
      "#include <map>\n",
      "using namespace std;\n",
      "\n",
      "int main() {\n",
      "    int a, b;\n",
      "    while (cin >> a >> b) {\n",
      "        int c = a + b;\n",
      "        int res = 0;\n",
      "        if (c == 0) {\n",
      "            cout << 1 << endl;\n",
      "            continue;\n",
      "        }\n",
      "        while (c) {\n",
      "            c /= 10;\n",
      "            res ++;\n",
      "        }\n",
      "     \n",
      "[/C++]\n",
      "[Response]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from prompt_utils import make_prompt_template, parse_code, cut_text, make_prompt_template_pd, parse_pd_html, make_simple_prompt_template\n",
    "from prompt_utils import parse_pd_html\n",
    "from pathlib import Path\n",
    "from prompt_utils import gemmaRegularPromptTemplate, baseInstructionTemplate, baseExampleTemplate, baseCodeTaskDescription, incompleteCodeTemplate\n",
    "\n",
    "#make instruction\n",
    "# instruction = f\"{baseInstructionTemplate} \\n {baseExampleTemplate} \\n {baseCodeTaskDescription(codeDescriptionString)}\"\n",
    "# gemmaRegularPromptTemplate(instruction = instruction,  response=\"\", code=incompleteCodeTemplate(codeString))\n",
    "\n",
    "def cut_text(txt: str, cut_ratio=0.1):\n",
    "    '''\n",
    "    Cut cut_ratio of the text from the end \n",
    "    '''\n",
    "    stripped = txt.strip('\\n ')\n",
    "    end_idx = max(int(cut_ratio * len(stripped)), 1)\n",
    "    return stripped[:-end_idx]\n",
    "\n",
    "def get_prompt_templates(samples_path: Path, include_pd: bool = False, include_example: bool = False):\n",
    "    prompt_templates = []\n",
    "\n",
    "    with open(samples_path) as samples_file:\n",
    "        samples = samples_file.readlines()\n",
    "        for sample in samples:\n",
    "\n",
    "            with open(sample.strip('\\n')) as f:\n",
    "                text = f.read()\n",
    "                code_sample = cut_text(text)\n",
    "\n",
    "            \n",
    "            instruction = f\"{baseInstructionTemplate()}\"\n",
    "\n",
    "            if include_example:\n",
    "                instruction += f\"\\n{baseExampleTemplate()}\"\n",
    "                # template = make_prompt_template(code_sample)\n",
    "            if(include_pd):\n",
    "                problem_id = sample.strip('\\n').split(\"/\")[-3]\n",
    "                pd_path = f\"../Project_CodeNet/problem_descriptions/{problem_id}.html\"\n",
    "                pd = parse_pd_html(pd_path)\n",
    "                instruction += f\"\\n{baseCodeTaskDescription(pd)}\"\n",
    "                # template = make_simple_prompt_template(code_sample, pd)\n",
    "\n",
    "\n",
    "            template = gemmaRegularPromptTemplate(instruction = instruction,  response=\"\", code=incompleteCodeTemplate(code_sample))\n",
    "\n",
    "            prompt_templates.append(template)\n",
    "            \n",
    "    return prompt_templates  \n",
    "\n",
    "prompt_templates = get_prompt_templates('./samples_small.txt', include_pd=True, include_example=True) #all\n",
    "\n",
    "# prompt_templates = get_prompt_templates('./samples_small.txt', include_pd=False, include_example=True)#include example\n",
    "\n",
    "# prompt_templates = get_prompt_templates('./samples_small.txt', include_pd=False, include_example=False)# no example no pd\n",
    "\n",
    "print(prompt_templates[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/cs/student/projects3/COMP0087/grp2/hf-hub\n",
      "env: TIKTOKEN_CACHE_DIR=/cs/student/projects3/COMP0087/grp2/tiktoken-cache\n",
      "/cs/student/projects3/COMP0087/grp2/hf-hub\n",
      "/cs/student/projects3/COMP0087/grp2/tiktoken-cache\n",
      "Experiment gemma-7b-bnb-4bit_ansible_test_6 already exists. Please choose an appropriate action:\n",
      "D - delete previous and overwrite. I - ignore. A - Abort\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME: /cs/student/projects3/COMP0087/grp2/hf-hub\n",
      "==((====))==  Unsloth: Fast Gemma patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090 Ti. Max memory: 23.677 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/msc/dsml/2023/gbillenn/NLP/miniconda3/envs/md_unsloth/lib/python3.10/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating faster inference with unsloth\n",
      "STARTING Inferance\n",
      "END Inferance\n"
     ]
    }
   ],
   "source": [
    "%set_env HF_HOME=/cs/student/projects3/COMP0087/grp2/hf-hub\n",
    "%set_env TIKTOKEN_CACHE_DIR=/cs/student/projects3/COMP0087/grp2/tiktoken-cache\n",
    "\n",
    "import os\n",
    "#print evn vars\n",
    "print(os.environ['HF_HOME'])\n",
    "print(os.environ['TIKTOKEN_CACHE_DIR'])\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import json \n",
    "import numpy as np\n",
    "\n",
    "from experiments import Experiment\n",
    "from prompt_utils import make_prompt_template, parse_code, cut_text, make_prompt_template_pd, parse_pd_html, make_simple_prompt_template\n",
    "from compiler_utils import try_compile_cpp\n",
    "import os\n",
    "import sys\n",
    "\n",
    "argument = \"gemma-7b-bnb-4bit\"\n",
    "exp_name = 6\n",
    "@dataclass\n",
    "class Evaluation:\n",
    "    experiment: Experiment \n",
    "    evaluation_id: str \n",
    "    prompt: str \n",
    "    output: str \n",
    "    code_output: Optional[str] = field(default=None)\n",
    "\n",
    "    @property\n",
    "    def eval_folder(self) -> Path:\n",
    "        return self.experiment.root_folder.joinpath(f'./evals/{self.evaluation_id}')\n",
    "\n",
    "    @property \n",
    "    def prompt_file_path(self) -> Path:\n",
    "        return self.eval_folder.joinpath('prompt.txt')\n",
    "\n",
    "    @property \n",
    "    def output_file_path(self) -> Path:\n",
    "        return self.eval_folder.joinpath('output.txt')\n",
    "\n",
    "    @property \n",
    "    def code_output_file_path(self) -> Path:\n",
    "        return self.eval_folder.joinpath('code_output.cpp')\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.code_output = parse_code(self.output)\n",
    "        self.output = self.prompt + self.output\n",
    "\n",
    "        to_write = [\n",
    "            (self.prompt_file_path, self.prompt),\n",
    "            (self.output_file_path, self.output),\n",
    "            (self.code_output_file_path, self.code_output)\n",
    "        ]\n",
    "        for file_path, text in to_write:\n",
    "            with open(file_path, 'w') as file:\n",
    "                if(text is None):\n",
    "                    text = \"None\"\n",
    "                file.write(text)\n",
    "\n",
    "\n",
    "prompt_templates = get_prompt_templates('./samples_small.txt', True)\n",
    "\n",
    "experiment = Experiment(\n",
    "    model=f\"unsloth/{argument}\",\n",
    "    exp_type='eval',\n",
    "    seq_length=8192,\n",
    "    max_new_tokens=1024,\n",
    "    exp_name=f'{argument}_ansible_test_{exp_name}'\n",
    ") \n",
    "\n",
    "model, tokenizer = experiment.get_unsloth_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.config.output_hidden_states = True # for contrastive search\n",
    "\n",
    "# print(model.config.output_hidden_states)\n",
    "\n",
    "#prompts = [\n",
    "#     tokenizer.apply_chat_template(template, tokenize=False)\n",
    "#     for template in prompt_templates \n",
    "#]\n",
    "prompts = prompt_templates\n",
    "\n",
    "# prompts = [prompt + tokenizer.eos_token for prompt in prompts]#!!!!!!!!!!!!!!!!!!!!!!!!!! add eos token to end\n",
    "\n",
    "sample_results = dict()\n",
    "\n",
    "# If this is too big we OOM for some reason...\n",
    "PROMPT_BATCH_SIZE = 2#5\n",
    "PROMPT_BATCHES = (len(prompts) + PROMPT_BATCH_SIZE - 1) // PROMPT_BATCH_SIZE\n",
    "\n",
    "#list of responses\n",
    "responses = []\n",
    "\n",
    "print(\"STARTING Inferance\")\n",
    "for prompt_batch_idx in range(PROMPT_BATCHES):\n",
    "    prompt_batch = prompts[prompt_batch_idx * PROMPT_BATCH_SIZE: (prompt_batch_idx + 1) * PROMPT_BATCH_SIZE]\n",
    "    inputs = tokenizer(prompt_batch, return_tensors = \"pt\", padding=True, truncation=True, max_length=8192).to('cuda')\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    outputs = model.generate(**inputs, \n",
    "                            max_new_tokens=100,#1024, \n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "                            # use_cache = True,\n",
    "\n",
    "                            #reduce chance for low ranked tokens\n",
    "                            #  do_sample=True,\n",
    "                            #  top_k=50,\n",
    "                            \n",
    "                            #lower temp -> more random - better for writing text\n",
    "                            # do_sample=True,\n",
    "                            # top_k=0, #deactive\n",
    "                            # temperature=0.6,\n",
    "\n",
    "                            # more dynamic, very heavy on Memory\n",
    "                            #  num_beams=5,#5,\n",
    "                            #  no_repeat_ngram_size=2,\n",
    "                            #  num_return_sequences=5,#5,\n",
    "                            #  early_stopping=True,\n",
    "\n",
    "                            #Top-p (nucleus) sampling\n",
    "                            do_sample=True,\n",
    "                            top_p=0.92,\n",
    "                            top_k=0,\n",
    "\n",
    "                            \n",
    "                            # papers:\n",
    "                            #Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models\n",
    "                            #https://github.com/martin-wey/peft-llm-code/blob/main/test.py#L127\n",
    "                            # num_beams=10,\n",
    "                            # num_return_sequences=10,\n",
    "                            # max_new_tokens=128#64 # args.max_target_length,\n",
    "                            # stopping_criteria=StoppingCriteriaList(\n",
    "                                # [EndOfFunctionCriteria(sample[\"input_ids\"].shape[1], eof_string, tokenizer)])\n",
    "\n",
    "                            #contrastive Search\n",
    "                            #https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need/blob/main/code_generation/inference.py\n",
    "                            #Needs: model.config.output_hidden_states = True\n",
    "                            #However this doesnt seem to be respected by the model\n",
    "                            # penalty_alpha=0.4, \n",
    "                            # top_k=3,\n",
    "                            \n",
    "                            #PPOCoder\n",
    "                            # do_sample=True,\n",
    "                            # top_p=1.0,\n",
    "                            # top_k=2,  \n",
    "\n",
    "                            #Codex\n",
    "                            # do_sample=True,\n",
    "                            # top_p=0.95,\n",
    "                            # top_k=0,                            \n",
    "\n",
    "                            #StepCoder - encoding\n",
    "                            # do_sample=True,\n",
    "                            # temperature=0.8,\n",
    "                            # top_p=0.9,\n",
    "                            # top_k=0\n",
    "                            #StepCoder - decoding\n",
    "                            # do_sample=True,\n",
    "                            # temperature=0.2,\n",
    "                            # top_p=0.95,\n",
    "                            # top_k=0\n",
    "                             )\n",
    "\n",
    "    for seq_idx, (prompt, output_sequence) in enumerate(zip(prompts, outputs)):\n",
    "        \n",
    "        #output = tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "        new_tokens = output_sequence[input_ids.shape[-1]:]\n",
    "\n",
    "        numpy_array = new_tokens.cpu().numpy()  \n",
    "\n",
    "        # Set NumPy's print options\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "        # print(numpy_array) \n",
    "        # print(new_tokens)\n",
    "        output = tokenizer.decode(new_tokens, skip_special_tokens=False)\n",
    "\n",
    "\n",
    "        responses.append({\"output\": output, \"numpy_array\":numpy_array, \"new_tokens\":new_tokens, \"output_sequence\":output_sequence, \"prompt\":prompt})\n",
    "\n",
    "        \n",
    "        # print(f\"##### {seq_idx} begin #####\")\n",
    "        # print(output)\n",
    "        # print(f\"##### {seq_idx} end #####\")\n",
    "        # seq_id = prompt_batch_idx * PROMPT_BATCH_SIZE + seq_idx \n",
    "        # evaluation = Evaluation(experiment, seq_id, prompt, output)\n",
    "        # compile_result = try_compile_cpp(src_path=evaluation.code_output_file_path)\n",
    "        # sample_status = \"Good\" if compile_result.returncode == 0 else \"Bad\"\n",
    "        # print(f'Sample {seq_id} is {sample_status}')\n",
    "        # sample_results[seq_id] = {\n",
    "        #     'Status': sample_status,\n",
    "        #     'stderr': compile_result.stderr.decode('utf-8')\n",
    "        # }\n",
    "\n",
    "    # with open(experiment.root_folder.joinpath('results.json'), 'a') as results_file:\n",
    "    #     json.dump(sample_results, results_file)\n",
    "        \n",
    "print(\"END Inferance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each of the responses:\n",
      "500\n",
      "453\n",
      "500\n",
      "292\n",
      "83\n",
      "\n",
      "----------------------------------------------- \n",
      "<bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos> \n",
      "\n",
      "\n",
      "----------------------------------------------- \n",
      "\tcout<<h[9]<<endl;\n",
      "\tcout<<h[<bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos> \n",
      "\n",
      "\n",
      "----------------------------------------------- \n",
      "<bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos> \n",
      "\n",
      "\n",
      "----------------------------------------------- \n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "8<bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos> \n",
      "\n",
      "\n",
      "----------------------------------------------- \n",
      "        3) << x << ' ' << y << std::endl;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "[/Response]\n",
      "[/C++]<eos> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the length of each of the responses\n",
    "print(\"Length of each of the responses:\")\n",
    "for response in responses:\n",
    "    print(len(response['output']))\n",
    "\n",
    "# for each response, print the output\n",
    "for response in responses:\n",
    "    string = f\"\\n----------------------------------------------- \\n{response['output']} \\n\"\n",
    "    print(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
